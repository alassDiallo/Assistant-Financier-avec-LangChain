{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alassDiallo/Assistant-Financier-avec-LangChain/blob/main/ChatGPT_Avec_Ses_Propres_PDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkgZa_eZ07O9"
      },
      "source": [
        "# Etapes de la procédure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FviX-UjD07O-"
      },
      "source": [
        "Étapes de la procédure:\n",
        "\n",
        "1. Installation des bibliothèques nécessaires:\n",
        "    - Nous commençons par installer toutes les bibliothèques nécessaires pour que notre script fonctionne. Cela inclut la gestion des PDFs, l'interaction avec OpenAI, l'indexation des vecteurs avec FAISS, et d'autres outils.\n",
        "\n",
        "2. Configuration des clés API:\n",
        "    - Ensuite, nous configurons l'accès à l'API OpenAI. Cela nous permettra d'utiliser les embeddings d'OpenAI pour nos besoins.\n",
        "\n",
        "3. Chargement du fichier PDF:\n",
        "    - Nous chargeons notre fichier PDF spécifique pour extraire le texte qu'il contient. Ceci sera la base de notre recherche et de notre interaction avec ChatGPT.\n",
        "\n",
        "4. Prétraitement du texte:\n",
        "    - Le texte extrait est ensuite divisé en morceaux plus petits pour s'assurer qu'il est gérable et pour éviter de dépasser les limites de taille des tokens.\n",
        "\n",
        "5. Création des embeddings:\n",
        "    - Nous convertissons ensuite ces morceaux de texte en embeddings (vecteurs) en utilisant OpenAI. Ces embeddings nous permettent de comparer la similitude entre différentes portions de texte et les questions posées.\n",
        "\n",
        "6. Indexation avec FAISS:\n",
        "    - Avec FAISS, nous indexons ces embeddings pour faciliter une recherche rapide. Lorsque nous posons une question, nous rechercherons les parties les plus pertinentes du texte en utilisant cet index.\n",
        "\n",
        "7. Mise en place de la chaîne de traitement de la réponse aux questions:\n",
        "    - Nous configurons la chaîne de traitement qui prendra nos questions, cherchera les textes pertinents, et générera des réponses à l'aide de ChatGPT.\n",
        "\n",
        "8. Interaction:\n",
        "    - Enfin, nous interagissons avec notre système en posant des questions, et le système nous donne des réponses basées sur le contenu du PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7-HzqKU07O-"
      },
      "source": [
        "# Importation des packages nécesseraire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIAZ1AzF07O-",
        "outputId": "607caf64-6e0e-4a63-99f7-eaac3441abb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "# Installer langchain, une bibliothèque pour les chaînes de traitement du langage naturel.\n",
        "!pip install langchain\n",
        "\n",
        "# Installer la bibliothèque openai pour interagir avec les modèles d'OpenAI, comme GPT-3.\n",
        "!pip install openai\n",
        "\n",
        "# Installer PyPDF2, une bibliothèque pour lire, écrire et manipuler les fichiers PDF en Python.\n",
        "!pip install PyPDF2\n",
        "\n",
        "# Installer faiss-cpu, une bibliothèque pour l'indexation efficace et la recherche de similitude des vecteurs.\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Installer tiktoken, une bibliothèque d'OpenAI pour compter les tokens dans un texte sans effectuer d'appel à l'API.\n",
        "!pip install tiktoken\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlorSbccWEDa"
      },
      "outputs": [],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "\n",
        "# PyPDF2 est une bibliothèque pour lire, écrire et manipuler les fichiers PDF en Python.\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# OpenAIEmbeddings est une classe de la bibliothèque langchain pour télécharger et gérer les embeddings d'OpenAI.\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# CharacterTextSplitter est une classe pour diviser de grands textes en morceaux plus petits, utile pour gérer des limites de tokens ou des contraintes de taille.\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# FAISS est une bibliothèque pour l'indexation efficace et la recherche de similitude des vecteurs. Ici, nous importons l'interface de langchain pour FAISS.\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# load_qa_chain est une fonction pour charger une chaîne de traitement préconfigurée pour répondre aux questions à partir de textes.\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# OpenAI est une classe pour interagir avec les modèles d'OpenAI, tels que GPT-3, via la bibliothèque langchain.\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# os est une bibliothèque standard de Python pour interagir avec le système d'exploitation, comme la gestion des variables d'environnement.\n",
        "import os\n",
        "\n",
        "\n",
        "# re est une bibliothèque standard de Python pour travailler avec les expressions régulières, utile pour la manipulation de texte et la recherche de motifs.\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJp39Y5d07O_"
      },
      "source": [
        "# Configurer l'API OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT5BQUmS07O_"
      },
      "source": [
        "\n",
        "\n",
        "Voici les étapes pour configurer l'API d'OpenAI :\n",
        "\n",
        "1. Inscription sur OpenAI\n",
        "Si vous n'avez pas encore de compte, inscrivez-vous sur [OpenAI](https://www.openai.com/).\n",
        "\n",
        "2. Obtention de la clé API\n",
        "    - Une fois connecté, rendez-vous dans la section **API** de votre tableau de bord.\n",
        "    - Générez une nouvelle clé API ou utilisez une clé existante.\n",
        "\n",
        "Pour générer sa clé API, il faut:\n",
        "    - Cliquer sur le lien de son profil\n",
        "    -Cliquer sur view API keys\n",
        "    - Create new secret key ensuite donner un nom et copier le code\n",
        "\n",
        "3. Ajouter une méthode de paiement, l'api ne fonctionne pas si vous avez 0 dollar (Billing)\n",
        "\n",
        "4. Installation de la bibliothèque OpenAI Python\n",
        "Installez le paquet officiel d'OpenAI pour Python. Vous pouvez le faire via pip :\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'VOTRE_CLÉ_API'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKaKB_GjWKjL"
      },
      "outputs": [],
      "source": [
        "# Configuration de l'API OpenAI\n",
        "# Assurez-vous d'avoir votre clé API avant de poursuivre.\n",
        "# Remplacez 'VOTRE_CLÉ_ICI' par votre clé.\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA92r1o007PA"
      },
      "source": [
        "# Importation et prétraitement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NalD3XkQWrJR"
      },
      "outputs": [],
      "source": [
        "# Lecture et extraction du texte à partir du fichier PDF\n",
        "# Assurez-vous que le fichier est présent dans le répertoire de travail ou fournissez le chemin complet\n",
        "chemin_pdf = 'societe-generale-pilier-3-t2-2022-fr.pdf'\n",
        "lecteur = PdfReader(chemin_pdf)\n",
        "\n",
        "lecteur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xcad7GJ07PA"
      },
      "outputs": [],
      "source": [
        "# Initialisation d'une chaîne vide pour stocker l'ensemble du texte extrait du PDF.\n",
        "texte_brut = ''\n",
        "\n",
        "# Boucle à travers chaque page du PDF. La fonction enumerate renvoie également un indice (i) pour chaque page.\n",
        "for i, page in enumerate(lecteur.pages):\n",
        "\n",
        "    # Extraction du texte de la page actuelle\n",
        "    text = page.extract_text()\n",
        "\n",
        "    # Vérification si du texte a été effectivement extrait de la page\n",
        "    if text:\n",
        "\n",
        "        # Ajout du texte extrait à la variable texte_brut\n",
        "        texte_brut += text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fh7DbZR07PA"
      },
      "outputs": [],
      "source": [
        "texte_brut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw_JoeHC07PA"
      },
      "source": [
        "Si vous utilisez des modèles de traitement de texte comme ceux basés sur l'architecture Transformer (e.g., BERT, GPT, etc.), ils ont une limite maximale quant au nombre de tokens qu'ils peuvent gérer en une seule fois. Par exemple, le modèle BERT de base peut gérer jusqu'à 512 tokens par entrée. Si votre texte dépasse cette limite, il doit être découpé pour être traité.\n",
        "\n",
        "Découper permet également de mieux générer la mémoire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9ASI0Vk07PA"
      },
      "outputs": [],
      "source": [
        "# Découpage du texte en morceaux plus petits pour éviter les limites de taille de token\n",
        "separateur = \"\\n\"\n",
        "decoupeur_texte = CharacterTextSplitter(\n",
        "    separator=separateur,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "textes = decoupeur_texte.split_text(texte_brut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfrF5Mf_07PA"
      },
      "outputs": [],
      "source": [
        "textes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziA1e1q707PA"
      },
      "outputs": [],
      "source": [
        "# Téléchargement des embeddings depuis OpenAI\n",
        "modeles_embedding = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbW1qaeP07PA"
      },
      "outputs": [],
      "source": [
        "# Initialisation de la structure de recherche FAISS (Facebook AI Similarity Search\") :\n",
        "# 1. Chaque texte de la liste 'textes' est transformé en un vecteur (embedding) en utilisant le modèle 'modeles_embedding'.\n",
        "# 2. Ces vecteurs sont ensuite stockés dans une structure de données FAISS optimisée pour la recherche rapide de similarité.\n",
        "# 'docsearch' sera utilisé pour retrouver rapidement les documents les plus similaires à une entrée donnée basée sur la similarité sémantique.\n",
        "docsearch = FAISS.from_texts(textes, modeles_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcnmBEu307PA"
      },
      "outputs": [],
      "source": [
        "# Chargement de la chaîne de traitement spécifique à la réponse aux questions (QA) :\n",
        "# 1. La fonction 'load_qa_chain' est utilisée pour initialiser une chaîne de traitement qui permet de répondre aux questions basées sur le contexte fourni.\n",
        "# 2. Elle utilise le modèle 'OpenAI()' pour générer les réponses.\n",
        "# 3. Le paramètre 'chain_type' spécifie le type ou la variante de la chaîne de traitement à charger, ici, c'est le type \"stuff\".\n",
        "# Une fois chargée, 'chain' peut être utilisée pour traiter des questions et obtenir des réponses pertinentes en fonction du contexte donné.\n",
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwbEBhd0ZUfX"
      },
      "outputs": [],
      "source": [
        "# Exemple de requête pour chercher des informations dans le document\n",
        "requete = \"quel était le montant xdes RWA à la société générale?\"\n",
        "documents_similaires = docsearch.similarity_search(requete)\n",
        "reponse = chain.run(input_documents=documents_similaires, question=requete)\n",
        "print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7NCCI0Af07PA"
      },
      "outputs": [],
      "source": [
        "# Exemple de requête pour chercher des informations dans le document\n",
        "requete = \"Quels sont les principaux indicateurs de performance financière mentionnés dans le document ?\"\n",
        "documents_similaires = docsearch.similarity_search(requete)\n",
        "reponse = chain.run(input_documents=documents_similaires, question=requete)\n",
        "print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtPIjRls07PA"
      },
      "outputs": [],
      "source": [
        "# Exemple de requête pour chercher des informations dans le document\n",
        "requete = \"Qui est l'auteur du document? \"\n",
        "documents_similaires = docsearch.similarity_search(requete)\n",
        "reponse = chain.run(input_documents=documents_similaires, question=requete)\n",
        "print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiUdGP3g07PA"
      },
      "outputs": [],
      "source": [
        "# Exemple de requête pour chercher des informations dans le document\n",
        "requete = \"Quels sont les risques mentionnés dans le document? \"\n",
        "documents_similaires = docsearch.similarity_search(requete)\n",
        "reponse = chain.run(input_documents=documents_similaires, question=requete)\n",
        "print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZsfnvTI07PA"
      },
      "outputs": [],
      "source": [
        "# Exemple de requête pour chercher des informations dans le document\n",
        "requete = \"Quel est le montant des fonds propres à la fin de la période de déclaration? \"\n",
        "documents_similaires = docsearch.similarity_search(requete)\n",
        "reponse = chain.run(input_documents=documents_similaires, question=requete)\n",
        "print(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e4Wpu9J07PA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vpf3Um07PB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}